from operator import methodcaller
from sklearn.datasets import load_boston, load_iris, load_breast_cancer, load_wine, fetch_covtype, load_digits
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, RobustScaler, MaxAbsScaler
from sklearn.preprocessing import QuantileTransformer, PowerTransformer #scaling 
# :: QuantileTransformer, RobustScaler ->이상치에 자유로움
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score, accuracy_score
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBClassifier, XGBRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor


#1. 데이터
path = './_data/kaggle_titanic/'
train_set = pd.read_csv(path + 'train.csv', # + 명령어는 문자를 앞문자와 더해줌
                        index_col=0) # index_col=n n번째 컬럼을 인덱스로 인식
# print(train_set)
# print(train_set.shape) # (891, 11)
# print(train_set.describe())
# print(train_set.columns)

test_set = pd.read_csv(path + 'test.csv', # 예측에서 쓸거임                
                       index_col=0)
# print(test_set)
# print(test_set.shape) # (418, 10)
# print(test_set.describe())

#print(train_set.Pclass.value_counts())


# print(train_set.isnull().sum()) #결측치를 전부 더한다
# from sklearn.experimental import enable_iterative_imputer
# from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
# imputer = KNNImputer()
# imputer.fit(train_set)
# data2 = imputer.transform(train_set)
# print(data2)
# print(train_set.isnull().sum()) # 없어졌는지 재확인

#print(train_set.columns)



#train_set = pd.DataFrame(train_set)



#=================컬럼별로 이상치 확인하고 제거해주기===============

def detect_outliers(df,n,features):
    outlier_indices = []  
    for col in features: 
        Q1 = np.percentile(df[col],25)
        Q3 = np.percentile(df[col],75)
        IQR = Q3 - Q1
        
        outlier_step = 1.5*IQR  
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        outlier_indices.extend(outlier_list_col)           
    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list( k for k , v in outlier_indices.items() if v > n)
        
    return multiple_outliers
    
Outliers_to_drop = detect_outliers(train_set,2,["Age",'SibSp','Parch','Fare'])

print(train_set.loc[Outliers_to_drop])
train_set.loc[Outliers_to_drop]
print(train_set.loc[Outliers_to_drop])
print(train_set.shape)
train_set = train_set.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)
print(train_set.shape)
print(train_set)


Pclass1 = train_set["Survived"][train_set["Pclass"] == 1].value_counts(normalize = True)[1]*100
Pclass2 = train_set["Survived"][train_set["Pclass"] == 2].value_counts(normalize = True)[1]*100
Pclass3 = train_set["Survived"][train_set["Pclass"] == 3].value_counts(normalize = True)[1]*100
print(f"Percentage of Pclass 1 who survived: {Pclass1}")
print(f"Percentage of Pclass 2 who survived: {Pclass2}")
print(f"Percentage of Pclass 3 who survived: {Pclass3}")

female = train_set["Survived"][train_set["Sex"] == 'female'].value_counts(normalize = True)[1]*100
male = train_set["Survived"][train_set["Sex"] == 'male'].value_counts(normalize = True)[1]*100
print(f"Percentage of females who survived: {female}")
print(f"Percentage of males who survived: {male}")

# df = pd.DataFrame(y)
# print(df)
# oh = OneHotEncoder(sparse=False) # sparse=true 는 매트릭스반환 False는 array 반환
# y = oh.fit_transform(df)
# print(y)

# print(test_set.columns)
# print(train_set.info()) # info 정보출력
# print(train_set.describe()) # describe 평균치, 중간값, 최소값 등등 출력

print(train_set.shape)



#### 결측치 처리 1. 제거 ####

train_set = train_set.fillna({"Embarked": "C"})
train_set.Age = train_set.Age.fillna(value=train_set.Age.mean())

train_set = train_set.drop(['Name'], axis = 1)
test_set = test_set.drop(['Name'], axis = 1)

train_set = train_set.drop(['Ticket'], axis = 1)
test_set = test_set.drop(['Ticket'], axis = 1)

train_set = train_set.drop(['Cabin'], axis = 1)
test_set = test_set.drop(['Cabin'], axis = 1)

train_set = pd.get_dummies(train_set,drop_first=True)
test_set = pd.get_dummies(test_set,drop_first=True)

test_set.Age = test_set.Age.fillna(value=test_set.Age.mean())
test_set.Fare = test_set.Fare.fillna(value=test_set.Fare.mode())

print(train_set, test_set, train_set.shape, test_set.shape)

############################


x = train_set.drop(['Survived'], axis=1)  # drop 데이터에서 ''사이 값 빼기
print(x)
print(x.columns)
print(x.shape) # (891, 8)

y = train_set['Survived'] 
print(y)
print(y.shape) # (891,)

x = np.array(x)

x_train, x_test, y_train, y_test = train_test_split(
    x, y, train_size=0.8, random_state=1234    
)

model_list = [RandomForestClassifier(), LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), KNeighborsClassifier()]


scalers = [StandardScaler(),MinMaxScaler(),
           MaxAbsScaler(),RobustScaler(),QuantileTransformer(),
           PowerTransformer(method = 'yeo-johnson'),
        #    PowerTransformer(method = 'box-cox')
           ]

for scaler in scalers : 
    name = str(scaler).strip('()')
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)
    #2. 모델
    print("==",name,"="*40)
    for model in model_list :
        model_name = str(model).strip('()')
        #3. 훈련
        model.fit(x_train, y_train)
        #4. 평가, 예측
        y_predict = model.predict(x_test)
        results = accuracy_score(y_test, y_predict)
        print(model_name,":: 의 결과 : ", round(results, 4))
    

'''
== StandardScaler ========================================
RandomForestClassifier :: 의 결과 :  0.7571
LogisticRegression :: 의 결과 :  0.7797
KNeighborsClassifier :: 의 결과 :  0.7458
DecisionTreeClassifier :: 의 결과 :  0.7288
KNeighborsClassifier :: 의 결과 :  0.7458
== MinMaxScaler ========================================
RandomForestClassifier :: 의 결과 :  0.7401
LogisticRegression :: 의 결과 :  0.7684
KNeighborsClassifier :: 의 결과 :  0.7627
DecisionTreeClassifier :: 의 결과 :  0.7288
KNeighborsClassifier :: 의 결과 :  0.7627
== MaxAbsScaler ========================================
RandomForestClassifier :: 의 결과 :  0.7458
LogisticRegression :: 의 결과 :  0.7684
KNeighborsClassifier :: 의 결과 :  0.7627
DecisionTreeClassifier :: 의 결과 :  0.7232
KNeighborsClassifier :: 의 결과 :  0.7627
== RobustScaler ========================================
RandomForestClassifier :: 의 결과 :  0.7401
LogisticRegression :: 의 결과 :  0.774
KNeighborsClassifier :: 의 결과 :  0.7514
DecisionTreeClassifier :: 의 결과 :  0.7175
KNeighborsClassifier :: 의 결과 :  0.7514
== QuantileTransformer ========================================
RandomForestClassifier :: 의 결과 :  0.7627
LogisticRegression :: 의 결과 :  0.7797
KNeighborsClassifier :: 의 결과 :  0.7345
DecisionTreeClassifier :: 의 결과 :  0.7062
KNeighborsClassifier :: 의 결과 :  0.7345
== PowerTransformer ========================================
RandomForestClassifier :: 의 결과 :  0.7684
LogisticRegression :: 의 결과 :  0.7797
KNeighborsClassifier :: 의 결과 :  0.7458
DecisionTreeClassifier :: 의 결과 :  0.7345
KNeighborsClassifier :: 의 결과 :  0.7458

'''


